{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMP/ATfeHwzSR5xrB2L6r07"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hB-s7sYsgQVp","executionInfo":{"status":"ok","timestamp":1671116219547,"user_tz":180,"elapsed":29317,"user":{"displayName":"leticia chen","userId":"04790660559346241686"}},"outputId":"3e1aceb6-d9b8-493a-ef12-8ca84da40aef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["# 請輸入資料夾之所在位置\n","FOLDERNAME = 'Colab\\ Notebooks/Amazon Fine Food'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\""],"metadata":{"id":"j2ucSk95ghbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))"],"metadata":{"id":"9ikVuO2Pg6di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get to the folder we are at\n","%cd drive/MyDrive/$FOLDERNAME/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZD3y3CXxg6bR","executionInfo":{"status":"ok","timestamp":1671116220557,"user_tz":180,"elapsed":1019,"user":{"displayName":"leticia chen","userId":"04790660559346241686"}},"outputId":"b5cc2be6-d16c-46bb-efc5-a4d071b1aac6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Amazon Fine Food\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install transformers[sentencepiece]"],"metadata":{"id":"EUcSo4ifvfDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('using device:', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Is9Y0F20hBMV","executionInfo":{"status":"ok","timestamp":1671116236324,"user_tz":180,"elapsed":2858,"user":{"displayName":"leticia chen","userId":"04790660559346241686"}},"outputId":"1b1c25c3-a595-41d1-e008-9bbe2d8899f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cuda\n"]}]},{"cell_type":"code","source":["if hasattr(torch.cuda, 'empty_cache'):\n","    torch.cuda.empty_cache()"],"metadata":{"id":"F47qnmXIkQN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","df = pd.read_csv('./Amazon_Fine_Food_Reviews.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":850},"id":"-vmpM5CwmH7l","executionInfo":{"status":"ok","timestamp":1671116242727,"user_tz":180,"elapsed":6413,"user":{"displayName":"leticia chen","userId":"04790660559346241686"}},"outputId":"0f411a21-c52a-45d2-beee-b9376e35b690"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            Id   ProductId          UserId                      ProfileName  \\\n","0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n","1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n","2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n","3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n","4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n","...        ...         ...             ...                              ...   \n","568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n","568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n","568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n","568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n","568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n","\n","        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n","0                          1                       1      5  1303862400   \n","1                          0                       0      1  1346976000   \n","2                          1                       1      4  1219017600   \n","3                          3                       3      2  1307923200   \n","4                          0                       0      5  1350777600   \n","...                      ...                     ...    ...         ...   \n","568449                     0                       0      5  1299628800   \n","568450                     0                       0      2  1331251200   \n","568451                     2                       2      5  1329782400   \n","568452                     1                       1      5  1331596800   \n","568453                     0                       0      5  1338422400   \n","\n","                                   Summary  \\\n","0                    Good Quality Dog Food   \n","1                        Not as Advertised   \n","2                    \"Delight\" says it all   \n","3                           Cough Medicine   \n","4                              Great taffy   \n","...                                    ...   \n","568449                 Will not do without   \n","568450                        disappointed   \n","568451            Perfect for our maltipoo   \n","568452  Favorite Training and reward treat   \n","568453                         Great Honey   \n","\n","                                                     Text  \n","0       I have bought several of the Vitality canned d...  \n","1       Product arrived labeled as Jumbo Salted Peanut...  \n","2       This is a confection that has been around a fe...  \n","3       If you are looking for the secret ingredient i...  \n","4       Great taffy at a great price.  There was a wid...  \n","...                                                   ...  \n","568449  Great for sesame chicken..this is a good if no...  \n","568450  I'm disappointed with the flavor. The chocolat...  \n","568451  These stars are small, so you can give 10-15 o...  \n","568452  These are the BEST treats for training and rew...  \n","568453  I am very satisfied ,product is as advertised,...  \n","\n","[568454 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-9cb0e04c-8561-4c31-bd5c-717e9ee5062d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>ProductId</th>\n","      <th>UserId</th>\n","      <th>ProfileName</th>\n","      <th>HelpfulnessNumerator</th>\n","      <th>HelpfulnessDenominator</th>\n","      <th>Score</th>\n","      <th>Time</th>\n","      <th>Summary</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>B001E4KFG0</td>\n","      <td>A3SGXH7AUHU8GW</td>\n","      <td>delmartian</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1303862400</td>\n","      <td>Good Quality Dog Food</td>\n","      <td>I have bought several of the Vitality canned d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>B00813GRG4</td>\n","      <td>A1D87F6ZCVE5NK</td>\n","      <td>dll pa</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1346976000</td>\n","      <td>Not as Advertised</td>\n","      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>B000LQOCH0</td>\n","      <td>ABXLMWJIXXAIN</td>\n","      <td>Natalia Corres \"Natalia Corres\"</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1219017600</td>\n","      <td>\"Delight\" says it all</td>\n","      <td>This is a confection that has been around a fe...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>B000UA0QIQ</td>\n","      <td>A395BORC6FGVXV</td>\n","      <td>Karl</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1307923200</td>\n","      <td>Cough Medicine</td>\n","      <td>If you are looking for the secret ingredient i...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>B006K2ZZ7K</td>\n","      <td>A1UQRSCLF8GW1T</td>\n","      <td>Michael D. Bigham \"M. Wassir\"</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1350777600</td>\n","      <td>Great taffy</td>\n","      <td>Great taffy at a great price.  There was a wid...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>568449</th>\n","      <td>568450</td>\n","      <td>B001EO7N10</td>\n","      <td>A28KG5XORO54AY</td>\n","      <td>Lettie D. Carter</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1299628800</td>\n","      <td>Will not do without</td>\n","      <td>Great for sesame chicken..this is a good if no...</td>\n","    </tr>\n","    <tr>\n","      <th>568450</th>\n","      <td>568451</td>\n","      <td>B003S1WTCU</td>\n","      <td>A3I8AFVPEE8KI5</td>\n","      <td>R. Sawyer</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1331251200</td>\n","      <td>disappointed</td>\n","      <td>I'm disappointed with the flavor. The chocolat...</td>\n","    </tr>\n","    <tr>\n","      <th>568451</th>\n","      <td>568452</td>\n","      <td>B004I613EE</td>\n","      <td>A121AA1GQV751Z</td>\n","      <td>pksd \"pk_007\"</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>1329782400</td>\n","      <td>Perfect for our maltipoo</td>\n","      <td>These stars are small, so you can give 10-15 o...</td>\n","    </tr>\n","    <tr>\n","      <th>568452</th>\n","      <td>568453</td>\n","      <td>B004I613EE</td>\n","      <td>A3IBEVCTXKNOH</td>\n","      <td>Kathy A. Welch \"katwel\"</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1331596800</td>\n","      <td>Favorite Training and reward treat</td>\n","      <td>These are the BEST treats for training and rew...</td>\n","    </tr>\n","    <tr>\n","      <th>568453</th>\n","      <td>568454</td>\n","      <td>B001LR2CU2</td>\n","      <td>A3LGQPJCZVL9UC</td>\n","      <td>srfell17</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1338422400</td>\n","      <td>Great Honey</td>\n","      <td>I am very satisfied ,product is as advertised,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>568454 rows × 10 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cb0e04c-8561-4c31-bd5c-717e9ee5062d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9cb0e04c-8561-4c31-bd5c-717e9ee5062d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9cb0e04c-8561-4c31-bd5c-717e9ee5062d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVuzcFrlolc4","executionInfo":{"status":"ok","timestamp":1671116243122,"user_tz":180,"elapsed":403,"user":{"displayName":"leticia chen","userId":"04790660559346241686"}},"outputId":"6240c517-e248-4874-9729-b72e58e0d8e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 568454 entries, 0 to 568453\n","Data columns (total 10 columns):\n"," #   Column                  Non-Null Count   Dtype \n","---  ------                  --------------   ----- \n"," 0   Id                      568454 non-null  int64 \n"," 1   ProductId               568454 non-null  object\n"," 2   UserId                  568454 non-null  object\n"," 3   ProfileName             568438 non-null  object\n"," 4   HelpfulnessNumerator    568454 non-null  int64 \n"," 5   HelpfulnessDenominator  568454 non-null  int64 \n"," 6   Score                   568454 non-null  int64 \n"," 7   Time                    568454 non-null  int64 \n"," 8   Summary                 568427 non-null  object\n"," 9   Text                    568454 non-null  object\n","dtypes: int64(5), object(5)\n","memory usage: 43.4+ MB\n"]}]},{"cell_type":"code","source":["reviews_data = pd.read_csv('./reviews_data.csv')\n","reviews_data.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPPVryVgNLIb","executionInfo":{"status":"ok","timestamp":1671116250893,"user_tz":180,"elapsed":7777,"user":{"displayName":"leticia chen","userId":"04790660559346241686"}},"outputId":"a608602d-dfa7-4222-ec64-b7d3ce868c43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 568454 entries, 0 to 568453\n","Data columns (total 4 columns):\n"," #   Column     Non-Null Count   Dtype \n","---  ------     --------------   ----- \n"," 0   Id         568454 non-null  int64 \n"," 1   ProductId  568454 non-null  object\n"," 2   Score      568454 non-null  int64 \n"," 3   Text       568454 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 17.3+ MB\n"]}]},{"cell_type":"code","source":["train = pd.read_csv('./train.csv')\n","seq_length = 0\n","for line in train.Text:\n","  tokens = line.split()\n","  num_token = len(tokens)\n","  if seq_length < num_token:\n","    seq_length = num_token\n","\n","print(seq_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZX2Nb7_Doif","executionInfo":{"status":"ok","timestamp":1671116261035,"user_tz":180,"elapsed":10154,"user":{"displayName":"leticia chen","userId":"04790660559346241686"}},"outputId":"1e6299a7-aa79-4783-9afb-57d7e8f20f06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3432\n"]}]},{"cell_type":"code","source":["# 01\n","import argparse\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='Using BERT for Amazon Fine Food analysis')\n","    parser.add_argument('corpus', type=str,\n","                        help='training corpus file')\n","    parser.add_argument('output_model', type=str,\n","                        help='output model file') \n","    parser.add_argument('--seq-length', type=int, default=512,\n","                        help='input sequence length (default: 512)')           \n","    parser.add_argument('--hidden-dim', type=int, default=256,\n","                        help='hidden dimension (default: 256)')\n","    parser.add_argument('--batch-size', type=int, default=10,\n","                        help='training batch size (default: 10)')                 \n","    parser.add_argument('--lr', type=float, default=0.0001,\n","                        help='learning rate (default: 0.0001)')                 \n","    parser.add_argument('--dropout', type=float, default=0.2,\n","                        help='dropout rate (default: 0.2)')\n","    parser.add_argument('--epochs', type=int, default=1,                       \n","                        help='number of epochs to train (default: 1)')\n","    parser.add_argument('--log-interval', type=int, default=10, \n","                        help='number of batches to wait before logging status (default: 10)') \n","    \n","    return parser.parse_args(args=['reviews_data.csv', 'output.pt'])  "],"metadata":{"id":"4VIKWVNAp6s2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 02\n","import csv\n","import torch\n","from torch.utils.data import Dataset\n","from transformers import AlbertTokenizer\n","\n","class ReviewsDataset(Dataset):\n","    def __init__(self, path, seq_length=512):\n","        super(ReviewsDataset).__init__()\n","        self.path = path\n","        self.seq_length = seq_length\n","        self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v1')     \n","\n","        self.parse_corpus()\n","\n","    def __len__(self):\n","        return self.n\n","\n","    def __getitem__(self, i):           \n","        return (self.input_ids[i], self.attention_mask[i], self.token_type_ids[i], self.y[i])\n","\n","    def parse_corpus(self):             \n","        '''\n","        Parse raw corpus text into input-output pairs, output tokenized by BertTokenizer\n","        '''\n","\n","        input_ids, attention_mask, token_type_ids, y = [], [], [], []\n","\n","        # Read csv from file\n","        product_set = set()                                 \n","        with open(self.path, 'r') as f:\n","            rd = csv.reader(f, delimiter=',')\n","            next(rd) # ignore header\n","            # count = 1\n","            for row in rd:\n","              # print(row)\n","              id, product_id, score, text = row                \n","              # print(id, product_id, score, text)\n","              # count += 1\n","              # if count == 10:\n","              #   break\n","              \n","              # Only keep complete sentences\n","              if product_id in product_set:\n","                  continue\n","              product_set.add(product_id)\n","\n","              # Tokenize each batch of phrases, truncate or pad to max length specified\n","              x = self.tokenizer(text, return_tensors='pt', max_length=self.seq_length, truncation=True, padding='max_length')  # pt stands for PyTorch\n","\n","              input_ids.append(x.input_ids.squeeze(0))\n","              attention_mask.append(x.attention_mask.squeeze(0))       \n","              token_type_ids.append(x.token_type_ids.squeeze(0))       \n","              y.append(int(score))\n","\n","        self.input_ids, self.attention_mask, self.token_type_ids, self.y = input_ids, attention_mask, token_type_ids, y\n","        self.n = len(y)"],"metadata":{"id":"7EKA8xw4yu_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 03\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import AlbertModel\n","\n","class BertFoodReviews(nn.Module):\n","    def __init__(self, n_class, hidden_dim=256, dropout=0.2):          \n","        super(BertFoodReviews, self).__init__()\n","\n","        self.enc = AlbertModel.from_pretrained('albert-base-v1')          \n","        # self.fc = nn.Linear(self.enc.config.hidden_size, hidden_dim)    \n","        # self.dropout = nn.Dropout(dropout)\n","        # self.out = nn.Linear(hidden_dim, n_class)\n","        self.out = nn.Linear(self.enc.config.hidden_size, n_class)      \n","\n","    def freeze_bert(self, freeze):                                        \n","        for param in self.enc.parameters():\n","          param.requires_grad = not freeze                                \n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):         \n","        output = self.enc(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        # y = self.fc(output.pooler_output)                               \n","        # y = F.relu(y)\n","        # y = self.dropout(y)\n","        # y = self.out(y)\n","        y = self.out(output.pooler_output)                                \n","        return y"],"metadata":{"id":"JVXJmuxtSEpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 04\n","def train(model, optimizer, data, args):              # data = train_dataloader\n","    model.train()\n","    #model.freeze_bert(True) # don't fine-tune bert to speedup model training\n","\n","    n_batch = len(data)                                   # batch size\n","    n_data = len(data.dataset)                        \n","    losses = []\n","    accs = []\n","    n_iters = 0\n","    for epoch in range(args.epochs):                  \n","        total_loss = 0\n","        total_correct = 0\n","        for batch_i, (input_ids, attention_mask, token_type_ids, target) in enumerate(data):\n","            input_ids = input_ids.to(device)                      \n","            attention_mask = attention_mask.to(device)            \n","            token_type_ids = token_type_ids.to(device)          \n","            target = target.to(device)\n","\n","            # Train\n","            optimizer.zero_grad()                                 # clear the previous G.D.\n","            output = model(input_ids, attention_mask, token_type_ids)     \n","            loss = F.cross_entropy(output, target)          # F = torch.nn.functional\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Log training status\n","            n_iters += 1\n","            total_loss += loss.item()\n","            pred = output.argmax(dim=1, keepdim=True)  \n","            correct = pred.eq(target.view_as(pred)).sum().item()\n","            total_correct += correct\n","             \n","            if batch_i % args.log_interval == 0:\n","                print('Train epoch: {} ({:2.0f}%)\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.0f}%)'.format(\n","                    epoch, 100. * batch_i / n_batch, loss.item(),\n","                    correct, len(target), 100. * correct / len(target)))\n","        losses.append(total_loss / len(data))\n","        accs.append(total_correct / n_data)"],"metadata":{"id":"dyjTOPjkNXbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 05\n","def test(model, data, args):\n","    model.eval()\n","\n","    n_batch = len(data)\n","    n_data = len(data.dataset)\n","    total_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for input_ids, attention_mask, token_type_ids, target in data:\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            token_type_ids = token_type_ids.to(device)\n","            target = target.to(device)\n","\n","            output = model(input_ids, attention_mask, token_type_ids)\n","            loss = F.cross_entropy(output, target)\n","            total_loss += loss.item()\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n","\n","    avg_loss = total_loss / n_batch\n","    print('Test Loss: {:.6f}'.format(avg_loss))\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        avg_loss, correct, n_data,\n","        100. * correct / n_data))"],"metadata":{"id":"BcX8tZa6s3VV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 06\n","import torch, gc\n","import transformers\n","from torch.utils.data import Dataset\n","from transformers import AlbertTokenizer\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split\n","\n","def main():\n","\n","  args = parse_args()\n","\n","  # Prepare data & split\n","  dataset = ReviewsDataset(args.corpus, seq_length=args.seq_length)\n","  train_set_size = int(len(dataset) * 0.8)                          \n","  train_set, val_set = random_split(dataset, [train_set_size, len(dataset) - train_set_size])\n","  train_dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)           \n","  val_dataloader = DataLoader(val_set, batch_size=args.batch_size)\n","  # torch.cuda.empty_cache()                                       \n","  print('Training set size: {}'.format(len(train_set)))           \n","  print('Val set size: {}'.format(len(val_set)))\n","  print('----------------------------')\n","\n","  # Create model & optimizer\n","  OUTPUT = 5\n","  model = BertFoodReviews(n_class = OUTPUT, hidden_dim=args.hidden_dim, dropout=args.dropout).to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","  # Train\n","  train(model, optimizer, train_dataloader, args)\n","\n","  # Save model\n","  torch.save(model.state_dict(), args.output_model)           \n","\n","  # Test\n","  test(model, val_dataloader, args)\n","\n","\n","if __name__ == '__main__':\n","  main()"],"metadata":{"id":"9Bf6ZmvFLTki"},"execution_count":null,"outputs":[]}]}